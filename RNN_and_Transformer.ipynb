{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6e9e8b",
   "metadata": {
    "id": "2a6e9e8b"
   },
   "source": [
    "# Recurrent Neural Networks and Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789993e",
   "metadata": {
    "id": "d789993e"
   },
   "source": [
    "## Text Classification with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92081dc6",
   "metadata": {
    "id": "92081dc6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import portalocker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = (AG_NEWS(split='train'))\n",
    "\n",
    "# Let's check what the data looks like\n",
    "print(len(train_iter))\n",
    "print(next(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb6bd7",
   "metadata": {
    "id": "28eb6bd7"
   },
   "source": [
    "### <font size='4'>Implement a RNNCell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db54f12",
   "metadata": {
    "id": "2db54f12"
   },
   "outputs": [],
   "source": [
    "# Documentation of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "class RNNCell(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RNNCell is a single cell that takes x_t and h_{t_1} as input and outputs h_t.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        \"\"\"\n",
    "        Constructor of RNNCell.\n",
    "        \n",
    "        Inputs: \n",
    "        - input_dim: Dimension of the input x_t\n",
    "        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n",
    "        \"\"\"\n",
    "        \n",
    "        # We always need to do this step to properly implement the constructor\n",
    "        super(RNNCell, self).__init__()\n",
    "        \n",
    "        self.linear_x, self.linear_h, self.non_linear = None, None, None  \n",
    "        \n",
    "        \n",
    "        # Define the linear transformation layers for x_t and h_{t-1} and the non-linear layer using tanh here.                            #\n",
    "        \n",
    "        self.linear_x = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear_h = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.non_linear = torch.nn.Tanh()\n",
    "       \n",
    "    def forward(self, x_cur: torch.Tensor, h_prev: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute h_t given x_t and h_{t-1}.\n",
    "        \n",
    "        Inputs:\n",
    "        - x_cur: x_t, a tensor with the same of BxC, where B is the batch size and \n",
    "          C is the channel dimension.\n",
    "        - h_prev: h_{t-1}, a tensor with the same of BxH, where H is the channel\n",
    "          dimension.\n",
    "        \"\"\"\n",
    "        h_cur = None\n",
    "        # Define the linear transformation layers for x_t and h_{t-1} and the non-linear layer.                                                   #\n",
    "        linear_x_output = self.linear_x(x_cur)\n",
    "        linear_h_output = self.linear_h(h_prev)\n",
    "\n",
    "        # Applying non-linear activation function\n",
    "        h_cur = self.non_linear(linear_x_output + linear_h_output)\n",
    "        return h_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f7e2d",
   "metadata": {
    "id": "b62f7e2d"
   },
   "outputs": [],
   "source": [
    "# Let's run a sanity check of your model\n",
    "x = torch.randn((2, 8))\n",
    "h = torch.randn((2, 16))\n",
    "model = RNNCell(8, 16)\n",
    "y = model(x, h)\n",
    "assert len(y.shape) == 2 and y.shape[0] == 2 and y.shape[1] == 16\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ee43c",
   "metadata": {
    "id": "cb2ee43c"
   },
   "source": [
    "### <font size='4'>Implement a single-layer (single-stack) RNN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0120521",
   "metadata": {
    "id": "f0120521"
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RNN is a single-layer (stack) RNN by connecting multiple RNNCell together in a single\n",
    "    direction, where the input sequence is processed from left to right.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        \"\"\"\n",
    "        Constructor of the RNN module.\n",
    "        \n",
    "        Inputs: \n",
    "        - input_dim: Dimension of the input x_t\n",
    "        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define the RNNCell.                                               \n",
    "        self.rnn_cell = RNNCell(input_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the hidden representations for every token in the input sequence.\n",
    "        \n",
    "        Input:\n",
    "        - x: A tensor with the shape of BxLxC, where B is the batch size, L is the squence \n",
    "          length, and C is the channel dimmension\n",
    "          \n",
    "        Return:\n",
    "        - h: A tensor with the shape of BxLxH, where H is the hidden dimension of RNNCell\n",
    "        \"\"\"\n",
    "        b = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        # initialize the hidden dimension\n",
    "        init_h = x.new_zeros((b, self.hidden_dim))\n",
    "        \n",
    "        h = []\n",
    "        # Compute the hidden representation for every token in the input from left to right.\n",
    "       \n",
    "        h_t = init_h\n",
    "        for t in range(seq_len):\n",
    "            h_t = self.rnn_cell(x[:, t, :], h_t)\n",
    "            h.append(h_t.unsqueeze(1))\n",
    "\n",
    "        h = torch.cat(h, dim=1)\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb661d8",
   "metadata": {
    "id": "dfb661d8"
   },
   "outputs": [],
   "source": [
    "# Let's run a sanity check of your model\n",
    "x = torch.randn((2, 10, 8))\n",
    "model = RNN(8, 16)\n",
    "y = model(x)\n",
    "assert len(y.shape) == 3\n",
    "for dim, dim_gt in zip(y.shape, [2, 10, 16]):\n",
    "    assert dim == dim_gt\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f8ec47",
   "metadata": {
    "id": "90f8ec47"
   },
   "source": [
    "### <font size='4'>Implement a RNN-based text classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42889c",
   "metadata": {
    "id": "ed42889c"
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A RNN-based classifier for text classification. It first converts tokens into word embeddings.\n",
    "    And then feeds the embeddings into a RNN, where the hidden representations of all tokens are\n",
    "    then averaged to get a single embedding of the sentence. It will be used as input to a linear\n",
    "    classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            vocab_size: int, embed_dim: int, rnn_hidden_dim: int, num_class: int, pad_token: int\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Inputs:\n",
    "        - vocab_size: Vocabulary size, indicating how many tokens we have in total.\n",
    "        - embed_dim: The dimension of word embeddings\n",
    "        - rnn_hidden_dim: The hidden dimension of the RNN.\n",
    "        - num_class: Number of classes.\n",
    "        - pad_token: The index of the padding token.\n",
    "        \"\"\"\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        # word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n",
    "        \n",
    "        self.rnn, self.fc = None, None\n",
    "        \n",
    "        # Define the RNN and the classification layer.                \n",
    "       \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=rnn_hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Classification layer\n",
    "        self.fc = nn.Linear(rnn_hidden_dim, num_class)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Get classification scores (logits) of the input.\n",
    "        \n",
    "        Input:\n",
    "        - text: Tensor with the shape of BxLxC.\n",
    "        \n",
    "        Return:\n",
    "        - logits: Tensor with the shape of BxK, where K is the number of classes\n",
    "        \"\"\"\n",
    "        \n",
    "        # get word embeddings\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        logits = None\n",
    "        # Compute logits of the input.                                    \n",
    "       \n",
    "        # RNN layer\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "\n",
    "        # Average the hidden representations of all tokens\n",
    "        avg_output = rnn_output.mean(dim=1)\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.fc(avg_output\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d2044",
   "metadata": {
    "id": "ce2d2044"
   },
   "outputs": [],
   "source": [
    "# Let's run a sanity check of your model\n",
    "#torch.cuda.is_available()\n",
    "vocab_size = 10\n",
    "embed_dim = 16\n",
    "rnn_hidden_dim = 32\n",
    "num_class = 3\n",
    "\n",
    "x = torch.arange(vocab_size).view(1, -1)\n",
    "x = torch.cat((x, x), dim=0)\n",
    "print('x.shape: {}'.format(x.shape))\n",
    "model = RNNClassifier(vocab_size, embed_dim, rnn_hidden_dim, num_class, 0)\n",
    "y = model(x)\n",
    "assert len(y.shape) == 2 and y.shape[0] == 2 and y.shape[1] == num_class\n",
    "print(y.shape)\n",
    "\n",
    "model = model.to('cuda:0')\n",
    "x = x.to('cuda:0')\n",
    "y = model(x)\n",
    "print(y.shape, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612890ad",
   "metadata": {
    "id": "612890ad"
   },
   "source": [
    "### Set up data related stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ea24e",
   "metadata": {
    "id": "c92ea24e"
   },
   "outputs": [],
   "source": [
    "# check here for details https://github.com/pytorch/text/blob/main/torchtext/data/utils.py#L52-#L166\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# check here for details https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab_factory.py#L65-L113\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# A tokenizer splits a input setence into a set of tokens, including those puncuation\n",
    "# For example\n",
    "# >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
    "# >>> tokens\n",
    "# >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Creates a vocab object which maps tokens to indices\n",
    "# Check here for details https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab.py\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "\n",
    "# The specified token will be returned when a out-of-vocabulary token is queried.\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "# The padding token we need to use\n",
    "# The returned indices are always in an array\n",
    "PAD_TOKEN = vocab(tokenizer('<pad>'))\n",
    "assert len(PAD_TOKEN) == 1\n",
    "PAD_TOKEN = PAD_TOKEN[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13c252",
   "metadata": {
    "id": "0f13c252"
   },
   "source": [
    "### <font size='4'>Collate Batched Data with Data Loaders</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd006c5",
   "metadata": {
    "id": "bdd006c5"
   },
   "outputs": [],
   "source": [
    "### Documentation of DataLoader https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "from torch.utils.data import DataLoader  \n",
    "\n",
    "# Merges a list of samples to form a mini-batch of Tensor(s)\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - batch: A list of data in a mini batch, where the length denotes the batch size. \n",
    "      The actual context depends on a particular dataset. In our case, each position \n",
    "      contains a label and a Tensor (tokens in a sentence).\n",
    "      \n",
    "    Returns:\n",
    "    - batched_label: A Tensor with the shape of (B,)\n",
    "    - batched_text: A Tensor with the shape of (B, L, C), where L is the sequence length\n",
    "      and C is the channeld dimension\n",
    "    \"\"\"\n",
    "    label_list, text_list, text_len_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        text_len_list.append(processed_text.size(0))\n",
    "    batched_label, batched_text = None, None\n",
    "    # Pad the text tensor in the mini batch so that they have the same length\n",
    "    # Specifically, you need to calculate the maximum length in the batch and then add the token PAD_TOKEN to the end of those shorter sentences.                                                      #\n",
    "    # find the max length of text in the mini-batch\n",
    "    max_len = max(text_len_list)\n",
    "\n",
    "    # create a tensor of zeros with the shape (batch_size, max_len, channels)\n",
    "    batched_text = torch.zeros((len(batch), max_len, channels))\n",
    "\n",
    "    # pad each sequence in the mini-batch with zeros to make them the same length\n",
    "    for i, text in enumerate(text_list):\n",
    "        padded_text = torch.cat((text, torch.zeros((max_len - text.size(0), channels), dtype=torch.int64)), dim=0)\n",
    "        batched_text[i] = padded_text\n",
    "\n",
    "    # create a tensor of labels for the mini-batch\n",
    "    batched_label = torch.tensor(label_list, dtype=torch.int64)\n",
    "    \n",
    "    return batched_label.long(), batched_text.long()\n",
    "\n",
    "# Now, let's check what the batched data looks like\n",
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "for idx, (label, data) in enumerate(dataloader):\n",
    "    if idx > 0:\n",
    "        break\n",
    "    print('label.shape: {}'.format(label.shape))\n",
    "    print('label: {}'.format(label))\n",
    "    print('data.shape: {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3571e",
   "metadata": {
    "id": "a5c3571e"
   },
   "source": [
    "### <font size='4'>Functions of training for a single epoch and evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc8d4a",
   "metadata": {
    "id": "b5dc8d4a"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataloader, loss_func, device, grad_norm_clip):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        label = label.to(device)\n",
    "        text = text.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = None\n",
    "        # compute the logits of the input, get the loss, and do the gradient backpropagation.\n",
    "    \n",
    "        logits = model(text)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_func(logits, label)\n",
    "\n",
    "        # do the gradient backpropagation\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "        optimizer.step()\n",
    "        total_acc += (logits.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model, dataloader, loss_func, device):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            label = label.to(device)\n",
    "            text = text.to(device)\n",
    "            # Compute the logits of the input, get the loss.                    \n",
    "            logits = model(text)\n",
    "            loss = loss_func(logits, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            total_acc += torch.sum(pred == label).item()\n",
    "            total_count += label.shape[0]\n",
    "            total_acc += (logits.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249cd2c",
   "metadata": {
    "id": "e249cd2c"
   },
   "source": [
    "### <font size='4' color='red'>Task 1.6: Define the model and loss function to train the model (3 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112151f",
   "metadata": {
    "id": "0112151f"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "# device = 'cuda'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyper parameters\n",
    "epochs = 3 # epoch\n",
    "lr = 0.0005 # learning rate\n",
    "batch_size = 64 # batch size for training\n",
    "word_embed_dim = 64\n",
    "rnn_hidden_dim = 96\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model, loss_func = None, None\n",
    "# Define the classifier\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, word_embed_dim, rnn_hidden_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, word_embed_dim)\n",
    "        self.rnn = nn.GRU(word_embed_dim, rnn_hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_dim, num_class)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        last_output = output[:, -1, :]\n",
    "        logits = self.fc(last_output)\n",
    "        return logits\n",
    "        \n",
    "model = TextClassificationModel(vocab_size, word_embed_dim, rnn_hidden_dim, num_class)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# copy the model to the specified device (GPU)\n",
    "model = model.to(device)\n",
    "        \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, \n",
    "    [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=batch_size,\n",
    "    shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=batch_size, \n",
    "    shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size,\n",
    "    shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# You should be able get a validation accuracy around 87%\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_dataloader, loss_func, device, 1)\n",
    "    accu_val = evaluate(model, valid_dataloader, loss_func, device)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3582578",
   "metadata": {
    "id": "e3582578"
   },
   "source": [
    "## Text Classification with Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1190d",
   "metadata": {
    "id": "4db1190d"
   },
   "source": [
    "### <font size='4'>Implement the multi-head attention module</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d932295",
   "metadata": {
    "id": "7d932295"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that computes multi-head attention given query, key, and value tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Inputs:\n",
    "        - input_dim: Dimension of the input query, key, and value. Here we assume they all have\n",
    "          the same dimensions. But they could have different dimensions in other problems.\n",
    "        - num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert input_dim % num_heads == 0\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_per_head = input_dim // num_heads\n",
    "        \n",
    "        # Define the linear transformation layers for key, value, and query.\n",
    "        # Also define the output layer.\n",
    "        self.key_layer = nn.Linear(input_dim, input_dim)\n",
    "        self.value_layer = nn.Linear(input_dim, input_dim)\n",
    "        self.query_layer = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        # Define the output layer.\n",
    "        self.output_layer = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Compute the attended feature representations.\n",
    "        \n",
    "        Inputs:\n",
    "        - query: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - key: Tensor of the shape BxLxC\n",
    "        - value: Tensor of the shape BxLxC\n",
    "        - mask: Tensor indicating where the attention should *not* be performed\n",
    "        \"\"\"\n",
    "        b = query.shape[0]        \n",
    "        \n",
    "        dot_prod_scores = None\n",
    "        # Compute the scores based on dot product between transformed query, key, and value. \n",
    "        # Reshape query, key, value so that multiple heads can be processed in parallel\n",
    "        Q = self.query_layer(query).view(b, -1, self.num_heads, self.dim_per_head)\n",
    "        K = self.key_layer(key).view(b, -1, self.num_heads, self.dim_per_head)\n",
    "        V = self.value_layer(value).view(b, -1, self.num_heads, self.dim_per_head)\n",
    "\n",
    "        # Transpose dimensions of Q, K, V to prepare for batch matrix multiplication\n",
    "        Q = Q.transpose(1,2) # Bxnum_headsxLxdim_per_head\n",
    "        K = K.transpose(1,2) # Bxnum_headsxLxdim_per_head\n",
    "        V = V.transpose(1,2) # Bxnum_headsxLxdim_per_head\n",
    "\n",
    "        # Perform batch matrix multiplication to calculate dot product between Q, K\n",
    "        dot_prod_scores = torch.matmul(Q, K.transpose(-2,-1)) # Bxnum_headsxLxL\n",
    "\n",
    "        # Scale the dot product scores by the square root of the dimension per head, as per the original paper\n",
    "        dot_prod_scores = dot_prod_scores / math.sqrt(self.dim_per_head\n",
    "        \n",
    "        if mask is not None:\n",
    "            # We simply set the similarity scores to be near zero for the positions\n",
    "            # where the attention should not be done. Think of why we do this.\n",
    "            dot_prod_scores = dot_prod_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        out = None\n",
    "        # Compute the attention scores, which are then used to modulate the value tensor. \n",
    "        # Finally concate the attended tensors from multiple heads and feed it into the output layer. \n",
    "        # Again, think of how to use reshaping tensor to do the concatenation.    #\n",
    "\n",
    "        #Compute the attention scores using softmax\n",
    "        attention_scores = F.softmax(dot_prod_scores, dim=-1) # shape: B x H x L x L\n",
    "        #Compute the attended tensor for each head\n",
    "        attended_tensors = []\n",
    "        for i in range(self.num_heads):\n",
    "            # shape: B x L x L\n",
    "            curr_att_scores = attention_scores[:, i, :, :]\n",
    "            # shape: B x L x d_head\n",
    "            curr_att_tensor = torch.matmul(curr_att_scores, value)\n",
    "            attended_tensors.append(curr_att_tensor)\n",
    "\n",
    "        # Step 4: Concatenate the attended tensors across all heads\n",
    "        concatenated_tensor = torch.cat(attended_tensors, dim=-1) # shape: B x L x C\n",
    "        \n",
    "        # Step 5: Apply output linear layer to get the final attended tensor\n",
    "        out = self.output_layer(concatenated_tensor)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd2c45",
   "metadata": {
    "id": "c5bd2c45"
   },
   "outputs": [],
   "source": [
    "x = torch.randn((2, 10, 8))\n",
    "mask = torch.randn((2, 10)) > 0.5\n",
    "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
    "num_heads = 4\n",
    "model = MultiHeadAttention(8, num_heads)\n",
    "y = model(x, x, x, mask)\n",
    "assert len(y.shape) == len(x.shape)\n",
    "for dim_x, dim_y in zip(x.shape, y.shape):\n",
    "    assert dim_x == dim_y\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02379104",
   "metadata": {
    "id": "02379104"
   },
   "source": [
    "### <font size='4'>Implement a Feedforward Network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7083ff",
   "metadata": {
    "id": "5b7083ff"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward network. Essentially, it is a two-layer fully-connected\n",
    "    neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, ff_dim, dropout):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension\n",
    "        - ff_dim: Hidden dimension\n",
    "        \"\"\"\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        # Define the two linear layers and a non-linear one.\n",
    "    \n",
    "        self.fc1 = nn.Linear(input_dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "         and C is the channel dimension\n",
    "          \n",
    "        Return:\n",
    "        - y: Tensor of the shape BxLxC\n",
    "        \"\"\"\n",
    "        \n",
    "        y = None\n",
    "        # Process the input.                                                #\n",
    "        y = self.dropout(x)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.fc2(y)\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671c24f",
   "metadata": {
    "id": "1671c24f"
   },
   "outputs": [],
   "source": [
    "x = torch.randn((2, 10, 8))\n",
    "ff_dim = 4\n",
    "model = FeedForwardNetwork(8, ff_dim, 0.1)\n",
    "y = model(x)\n",
    "assert len(x.shape) == len(y.shape)\n",
    "for dim_x, dim_y in zip(x.shape, y.shape):\n",
    "    assert dim_x == dim_y\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa64217",
   "metadata": {
    "id": "daa64217"
   },
   "source": [
    "### <font size='4'>Implement a Single Transformer Encoder Cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aca2a1",
   "metadata": {
    "id": "c5aca2a1"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) for the Transformer encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderCell, self).__init__()\n",
    "        \n",
    "        # A single Transformer encoder cell consists of \n",
    "        # 1. A multi-head attention module\n",
    "        # 2. Followed by dropout\n",
    "        # 3. Followed by layer norm (check nn.LayerNorm)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm\n",
    "        #                                                                         #\n",
    "        # At the same time, it also has\n",
    "        # 1. A feedforward network\n",
    "        # 2. Followed by dropout\n",
    "        # 3. Followed by layer norm\n",
    " \n",
    "        # 1. Multi-head attention module\n",
    "        self.self_attention = MultiHeadAttention(input_dim=input_dim, num_heads=num_heads)\n",
    "        \n",
    "        # Followed by dropout\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Followed by layer norm\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=input_dim)\n",
    "        \n",
    "        # 2. Feedforward network\n",
    "        self.feedforward = FeedForwardNetwork(input_dim=input_dim, ff_dim=ff_dim, dropout=dropout)\n",
    "        \n",
    "        # Followed by dropout\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Followed by layer norm\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=input_dim)\n",
    " \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \"\"\"\n",
    "        \n",
    "        y = None\n",
    "        # Get the output of the multi-head attention part (with dropout and layer norm), which is used as input to the feedforward network \n",
    "        # again, followed by dropout and layer norm).                                               \n",
    "        \n",
    "        # Multi-head attention part\n",
    "        attn_output = self.self_attention(x, x, x, mask=mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        # Residual connection\n",
    "        y = x + attn_output\n",
    "        y = self.norm1(y)\n",
    "        \n",
    "        # Feedforward network part\n",
    "        ff_output = self.feedforward(y)\n",
    "        ff_output = self.dropout2(ff_output)\n",
    "        # Residual connection\n",
    "        y = x + ff_output\n",
    "        y = self.norm2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4fcff",
   "metadata": {
    "id": "77e4fcff"
   },
   "outputs": [],
   "source": [
    "x = torch.randn((2, 10, 8))\n",
    "mask = torch.randn((2, 10)) > 0.5\n",
    "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
    "num_heads = 4\n",
    "model = TransformerEncoderCell(8, num_heads, 32, 0.1)\n",
    "y = model(x, mask)\n",
    "assert len(x.shape) == len(y.shape)\n",
    "for dim_x, dim_y in zip(x.shape, y.shape):\n",
    "    assert dim_x == dim_y\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0944cd8",
   "metadata": {
    "id": "b0944cd8"
   },
   "source": [
    "### <font size='4'>Implement Transformer Encoder</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b398a",
   "metadata": {
    "id": "f72b398a"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A full encoder consisting of a set of TransformerEncoderCell.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: Number of TransformerEncoderCells\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.norm = None\n",
    "        # Construct a nn.ModuleList to store a stack of TranformerEncoderCells. Check the documentation here of how to use it\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n",
    "        \n",
    "        # At the same time, define a layer normalization layer to process the output of the entire encoder.                                           \n",
    "\n",
    "        self.encoder_cells = nn.ModuleList([\n",
    "            TransformerEncoderCell(input_dim, num_heads, ff_dim, dropout) for _ in range(num_cells)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \n",
    "        Return:\n",
    "        - y: Tensor of the shape of BxLxC, which is the normalized output of the encoder\n",
    "        \"\"\"\n",
    "        \n",
    "        y = None\n",
    "        # Feed x into the stack of TransformerEncoderCells and then normalize the output with layer norm.                                   #\n",
    "\n",
    "        # Apply each encoder cell to the input sequentially\n",
    "        for cell in self.encoder_cells:\n",
    "          x = cell(x, mask)\n",
    "        # Apply layer normalization\n",
    "        y = self.norm(x)\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddad9c",
   "metadata": {
    "id": "64ddad9c"
   },
   "outputs": [],
   "source": [
    "x = torch.randn((2, 10, 8))\n",
    "mask = torch.randn((2, 10)) > 0.5\n",
    "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
    "num_heads = 4\n",
    "model = TransformerEncoder(8, num_heads, 32, 2, 0.1)\n",
    "y = model(x)\n",
    "assert len(x.shape) == len(y.shape)\n",
    "for dim_x, dim_y in zip(x.shape, y.shape):\n",
    "    assert dim_x == dim_y\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4330c",
   "metadata": {
    "id": "b4c4330c"
   },
   "source": [
    "### <font size='4'>Implement Positional Encoding</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5ff70",
   "metadata": {
    "id": "52b5ff70"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that adds positional encoding to each of the token's features.\n",
    "    So that the Transformer is position aware.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, max_len: int=10000):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension about the features for each token\n",
    "        - max_len: The maximum sequence length\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the positional encoding and add it to x.\n",
    "        \n",
    "        Input:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "          \n",
    "        Return:\n",
    "        - x: Tensor of the shape BxLxC, with the positional encoding added to the input\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        input_dim = x.shape[2]\n",
    "        \n",
    "        pe = None\n",
    "        # Compute the positional encoding                                   \n",
    "        # Check Section 3.5 for the definition (https://arxiv.org/pdf/1706.03762.pdf)\n",
    "                                        \n",
    "        # PE_{(pos,2i)} = sin(pos / 10000^{2i/\\dmodel})                           \n",
    "        # PE_{(pos,2i+1)} = cos(pos / 10000^{2i/\\dmodel})                         \n",
    "                                                                       \n",
    "        # You should replace 10000 with max_len here.\n",
    "    \n",
    "\n",
    "        # Create the positional encoding matrix\n",
    "        pe = torch.zeros(self.max_len, input_dim)\n",
    "        position = torch.arange(0, self.max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, input_dim, 2).float() * (-math.log(self.max_len) / input_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # Add batch dimension\n",
    "        self.register_buffer('pe', pe)\n",
    "        x = x + pe.to(x.device)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d76c1",
   "metadata": {
    "id": "6d1d76c1"
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "x = torch.randn(1, 100, 20)\n",
    "pe = PositionalEncoding(20)\n",
    "y = pe(x)\n",
    "assert len(x.shape) == len(y.shape)\n",
    "for dim_x, dim_y in zip(x.shape, y.shape):\n",
    "    assert dim_x == dim_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34a7de",
   "metadata": {
    "id": "5a34a7de"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward((torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403b5eb6",
   "metadata": {
    "id": "403b5eb6"
   },
   "source": [
    "### <font size='4'>Implement a Transformer-based Text Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75af7f",
   "metadata": {
    "id": "ea75af7f"
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-based text classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            vocab_size: int, embed_dim: int, num_heads: int, trx_ff_dim: int, \n",
    "            num_trx_cells: int, num_class: int, dropout: float=0.1, pad_token: int=0\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - vocab_size: Vocabulary size, indicating how many tokens we have in total.\n",
    "        - embed_dim: The dimension of word embeddings\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - trx_ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_trx_cells: Number of TransformerEncoderCells\n",
    "        - dropout: Dropout ratio\n",
    "        - pad_token: The index of the padding token.\n",
    "        \"\"\"\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n",
    "        \n",
    "        # Define a module for positional encoding, Transformer encoder, and a output layer                                                          #\n",
    "        # positional encoding layer\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=trx_ff_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_trx_cells)\n",
    "        \n",
    "        # output layer\n",
    "        self.output_layer = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - text: Tensor with the shape of BxLxC.\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \n",
    "        Return:\n",
    "        - logits: Tensor with the shape of BxK, where K is the number of classes\n",
    "        \"\"\"\n",
    "        \n",
    "        # word embeddings, note we multiple the embeddings by a factor\n",
    "        embedded = self.embedding(text) * math.sqrt(self.embed_dim)\n",
    "        \n",
    "        logits = None\n",
    "        # Apply positional embedding to the input, which is then fed into the encoder.\n",
    "        # Average pooling is applied then to all the features of all tokens.\n",
    "        # Finally, the logits are computed based on the pooled features.  \n",
    "\n",
    "        \n",
    "        # positional encoding\n",
    "        embedded = self.pos_enc(embedded)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        transformer_output = self.transformer_encoder(embedded, src_key_padding_mask=mask)\n",
    "        \n",
    "        # average pooling\n",
    "        pooled = torch.mean(transformer_output, dim=1)\n",
    "        \n",
    "        # output layer\n",
    "        logits = self.output_layer(pooled)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e926e90",
   "metadata": {
    "id": "5e926e90"
   },
   "outputs": [],
   "source": [
    "vocab_size = 10\n",
    "embed_dim = 16\n",
    "num_heads = 4\n",
    "trx_ff_dim = 16\n",
    "num_trx_cells = 2\n",
    "num_class = 3\n",
    "\n",
    "x = torch.arange(vocab_size).view(1, -1)\n",
    "x = torch.cat((x, x), dim=0)\n",
    "mask = (x != 0).unsqueeze(-2).unsqueeze(1)\n",
    "model = TransformerClassifier(vocab_size, embed_dim, num_heads, trx_ff_dim, num_trx_cells, num_class)\n",
    "print('x: {}, mask: {}'.format(x.shape, mask.shape))\n",
    "y = model(x, mask)\n",
    "assert len(y.shape) == 2 and y.shape[0] == x.shape[0] and y.shape[1] == num_class\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f02fc",
   "metadata": {
    "id": "8c1f02fc"
   },
   "source": [
    "### <font size='4'>Define the Model and Loss Function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd5779",
   "metadata": {
    "id": "46bd5779"
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "# device = 'cuda'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 5 # epoch\n",
    "lr = 0.0005  # learning rate\n",
    "batch_size = 64 # batch size for training\n",
    "  \n",
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "\n",
    "num_heads = 4\n",
    "num_trx_cells = 2\n",
    "\n",
    "gradient_norm_clip = 1\n",
    "\n",
    "# Define a Transformer-based text classifier and a loss function.         \n",
    "\n",
    "# Define the model and loss function\n",
    "model = TransformerClassifier(vocab_size, emsize, num_heads, 4*emsize, num_trx_cells, num_class).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n",
    "total_accu = None\n",
    "\n",
    "# You should be able to get a validation accuracy around 89%\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_dataloader, loss_func, device, gradient_norm_clip)\n",
    "    accu_val = evaluate(model, valid_dataloader, loss_func, device)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2a914",
   "metadata": {
    "id": "fba2a914"
   },
   "source": [
    "## Image Classification with Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d670e6",
   "metadata": {
    "id": "b8d670e6"
   },
   "source": [
    "### <font size='4'>Implement VisionTransformer for Image Classification</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87161192",
   "metadata": {
    "id": "87161192"
   },
   "outputs": [],
   "source": [
    "class VisionTransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    In the model, we partition an image into non-overlapping patches. Each patch is treated as a token.\n",
    "    We can get a sequence of such tokens by flattening the patches. Each token's embeddings is the\n",
    "    flattened RGB pixel values. If the patch size is 4, then the embeddings' dimension is 4*4*3.\n",
    "    You can check this paper https://arxiv.org/pdf/2010.11929.pdf for reference.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            patch_size: int, num_heads: int, trx_ff_dim: int, \n",
    "            num_trx_cells: int, num_class: int, dropout: float=0.1\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - patch_size: Size of the non-overlapping patches\n",
    "        - num_heads: Number of attention heads\n",
    "        - trx_ff_dim: Hidden dimension of the feedforward network in a Transformer encoder\n",
    "        - num_trx_cells: Number of TransformerEncoderCells \n",
    "        - num_class: Number of image classes\n",
    "        - dropout: Dropout ratio\n",
    "        \"\"\"\n",
    "        super(VisionTransformerClassifier, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Define a TransformerEncoder that takens non-overlapping patches of an image as input and another output layer for classification.       #\n",
    "\n",
    "        # Intuitively, we need 2D positional encodings for each patch according to its x and y coordinates.\n",
    "        # But this reference paper https://arxiv.org/pdf/2010.11929.pdf hows there is no significance difference on accuracies.\n",
    "\n",
    "        # Create the embedding layer for the patches\n",
    "        self.embedding_dim = patch_size * patch_size * 3\n",
    "        self.num_patches = (224 // patch_size) ** 2  # assuming 224x224 input size\n",
    "        \n",
    "        # Patch embedding layer\n",
    "        self.patch_embedding = nn.Conv2d(in_channels=3, \n",
    "                                         out_channels=self.embedding_dim, \n",
    "                                         kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Positional encoding layer\n",
    "        self.positional_encoding = PositionalEncoding(self.embedding_dim, dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.embedding_dim, \n",
    "                                                   nhead=num_heads, \n",
    "                                                   dim_feedforward=trx_ff_dim, \n",
    "                                                   dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_trx_cells)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(self.embedding_dim, num_class)\n",
    "    #def init_weights(self):\n",
    "          #initrange = 0.5\n",
    "          #self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "          #self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "          #self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - image: Tensor of the shape BxCxHxW, where H and W are the height and width, respectively.\n",
    "        \n",
    "        Return:\n",
    "        - logtis: Classification logits\n",
    "        \"\"\"\n",
    "        \n",
    "        b, c, h, w = image.shape\n",
    "        \n",
    "        # Partition an image into non-overlapping patches. \n",
    "        # Think of how to reshape the tensor to convert it to be the BxLxC format, which we have  \n",
    "        # extensively used for NLP tasks. You will find tensor.permute helpful.    \n",
    "        # Check documentation here https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute\n",
    "        \n",
    "        # Partition image into non-overlapping patches\n",
    "        patches = self.patch_embedding(image)  # shape: B x C x H' x W', where H' and W' depend on patch_size\n",
    "        patches = patches.flatten(2).transpose(1, 2)  # shape: B x L x C, where L = H' x W'\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        patches = self.positional_encoding(patches)\n",
    "        \n",
    "        # Apply Transformer encoder\n",
    "        patches = self.transformer_encoder(patches)\n",
    "        \n",
    "        # Apply output layer\n",
    "        logits = self.output_layer(patches.mean(dim=1))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a5642",
   "metadata": {
    "id": "1c0a5642"
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "image = torch.randn((2, 3, 32, 32))\n",
    "patch_size = 4\n",
    "num_heads = 4\n",
    "num_trx_cells = 2\n",
    "trx_ff_dim = 16\n",
    "dropout = 0.1\n",
    "num_class = 5\n",
    "\n",
    "vit = VisionTransformerClassifier(patch_size, num_heads, trx_ff_dim, num_trx_cells, num_class, dropout)\n",
    "logits = vit(image)\n",
    "assert len(logits.shape) == 2 and logits.shape[0] == image.shape[0] and logits.shape[1] == num_class\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966276ca",
   "metadata": {
    "id": "966276ca"
   },
   "source": [
    "### Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c3d63",
   "metadata": {
    "id": "b80c3d63"
   },
   "outputs": [],
   "source": [
    "# let's download the data\n",
    "%cd ../datasets\n",
    "\n",
    "# 1 -- Linux \n",
    "# 2 -- MacOS\n",
    "# 3 -- Command Prompt on Windows\n",
    "# 4 -- manually downloading the data\n",
    "choice = 1\n",
    "\n",
    "\n",
    "if choice == 1:\n",
    "    # should work well on Linux and in Powershell on Windows\n",
    "    !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "elif choice == 2 or choice ==3:\n",
    "    # if wget is not available for you, try curl\n",
    "    # should work well on MacOS\n",
    "    !curl http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz --output cifar-10-python.tar.gz\n",
    "else:\n",
    "    print('Please manually download the data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and put it under the datasets folder.')\n",
    "!tar -xzvf cifar-10-python.tar.gz\n",
    "\n",
    "if choice==3:\n",
    "    !del cifar-10-python.tar.gz\n",
    "else:\n",
    "    !rm cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c00ec",
   "metadata": {
    "id": "724c00ec"
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "import platform\n",
    "\n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = load_pickle(f)\n",
    "    X = datadict['data']\n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for b in range(1,6):\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)\n",
    "  Xtr = np.concatenate(xs)\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "  return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "\n",
    "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000,\n",
    "                     subtract_mean=True):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for classifiers. These are the same steps as we used for the SVM, but\n",
    "    condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    if subtract_mean:\n",
    "      mean_image = np.mean(X_train, axis=0)\n",
    "      X_train -= mean_image\n",
    "      X_val -= mean_image\n",
    "      X_test -= mean_image\n",
    "\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    # Package data into a dictionary\n",
    "    return {\n",
    "      'X_train': X_train, 'y_train': y_train,\n",
    "      'X_val': X_val, 'y_val': y_val,\n",
    "      'X_test': X_test, 'y_test': y_test,\n",
    "    }\n",
    "\n",
    "# Split the data into train, val, and test sets. \n",
    "# Check the get_CIFAR10_data function for more details\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "data = get_CIFAR10_data(cifar10_dir)\n",
    "for k, v in list(data.items()):\n",
    "    print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770e8d3",
   "metadata": {
    "id": "f770e8d3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import TensorDataset\n",
    "\n",
    "def make_dataloader(x, y, batch_size, is_train):\n",
    "    dataset = TensorDataset(\n",
    "        torch.from_numpy(y).long(),\n",
    "        torch.from_numpy(x).float() \n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=is_train,\n",
    "        num_workers=2,\n",
    "        drop_last=is_train\n",
    "    )\n",
    "    return dataloader\n",
    "    \n",
    "train_loader = make_dataloader(data['X_train'], data['y_train'], 8, True)\n",
    "for idx, (lab, im) in enumerate(train_loader):\n",
    "    if idx > 1:\n",
    "        break\n",
    "    print(im.shape, lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d32af",
   "metadata": {
    "id": "570d32af"
   },
   "source": [
    "### <font size='4'>Define the Model and Loss Function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f44a98",
   "metadata": {
    "id": "81f44a98"
   },
   "outputs": [],
   "source": [
    "patch_size = 4\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "trx_ff_dim = 128\n",
    "num_trx_cells = 2\n",
    "num_class = 10\n",
    "\n",
    "# Define the model and loss function\n",
    "\n",
    "# Define the model\n",
    "model = VisionTransformerClassifier(patch_size=patch_size,\n",
    "                                    num_heads=num_heads,\n",
    "                                    trx_ff_dim=trx_ff_dim,\n",
    "                                    num_trx_cells=num_trx_cells,\n",
    "                                    num_class=num_class)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "model = model.to(device)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = make_dataloader(data['X_train'], data['y_train'], batch_size, True)\n",
    "val_loader = make_dataloader(data['X_test'], data['y_test'], batch_size, False)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 5 # epoch\n",
    "lr = 0.001\n",
    "gradient_norm_clips = 0.1\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n",
    "total_accu = None\n",
    "\n",
    "# You should be able to get an accuracy around 36%\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_loader, loss_func, device, gradient_norm_clip)\n",
    "    accu_val = evaluate(model, val_loader, loss_func, device)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6ea7e",
   "metadata": {
    "id": "f0e6ea7e"
   },
   "source": [
    "## Machine Translation with Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad5d07",
   "metadata": {
    "id": "e6ad5d07"
   },
   "source": [
    "### <font size='4'>Implement Transformer Decoder Cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32619440",
   "metadata": {
    "id": "32619440"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) of the Transformer decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderCell, self).__init__()\n",
    "        \n",
    "        # Similar to the TransformerEncoderCell, define two MultiHeadAttention modules.\n",
    "        # One for processing the tokens on the decoder side.\n",
    "        # The other for getting the attention across the encoder and the decoder. \n",
    "        # Also define a feedforward network. \n",
    "        # Don't forget the Dropout and Layer Norm layers.                                        \n",
    "    \n",
    "        # Multi-head attention layer for the decoder\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=input_dim, \n",
    "                                                num_heads=num_heads, \n",
    "                                                dropout=dropout)\n",
    "        self.self_attn_ln = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Multi-head attention layer for the encoder-decoder attention\n",
    "        self.enc_dec_attn = nn.MultiheadAttention(embed_dim=input_dim, \n",
    "                                                  num_heads=num_heads, \n",
    "                                                  dropout=dropout)\n",
    "        self.enc_dec_attn_ln = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Feedforward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, input_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ffn_ln = nn.LayerNorm(input_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask=None, tgt_mask=None):            \n",
    "        \"\"\"\n",
    "        Inputs: \n",
    "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
    "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
    "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
    "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
    "        \n",
    "        Return:\n",
    "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
    "        \"\"\"\n",
    "        \n",
    "        y = None\n",
    "        # Compute the self-attended features for the tokens on the decoder side.\n",
    "        # Then compute the corss-attended features for the tokens on the decoder side to the encoded features, which are finally feed into the\n",
    "        # feedforward network                                                     \n",
    " \n",
    "        \n",
    "        # Self-attention\n",
    "        self_att, _ = self.self_attn(x, x, x, attn_mask=tgt_mask)\n",
    "        x = self.self_attn_ln(x + self_att)\n",
    "        \n",
    "        # Encoder-decoder attention\n",
    "        enc_dec_att, _ = self.enc_dec_attn(x, encoder_output, encoder_output, attn_mask=src_mask)\n",
    "        x = self.enc_dec_attn_ln(x + enc_dec_att)\n",
    "        \n",
    "        # Feedforward network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.ffn_ln(x + ffn_output)\n",
    "        y = x\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe2ae8",
   "metadata": {
    "id": "f1fe2ae8"
   },
   "outputs": [],
   "source": [
    "dec_feats = torch.randn((3, 10, 16))\n",
    "dec_mask = torch.randn((3, 1, 10, 10)) > 0.5\n",
    "\n",
    "enc_feats = torch.randn((3, 12, 16))\n",
    "enc_mask = torch.randn((3, 1, 1, 12)) > 0.5\n",
    "\n",
    "model = TransformerDecoderCell(16, 2, 32, 0.1)\n",
    "z = model(dec_feats, enc_feats, enc_mask, dec_mask)\n",
    "assert len(z.shape) == len(dec_feats.shape)\n",
    "for dim_z, dim_x in zip(z.shape, dec_feats.shape):\n",
    "    assert dim_z == dim_x\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36259c71",
   "metadata": {
    "id": "36259c71"
   },
   "source": [
    "### <font size='4'>Implement Transformer Decoder</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a3d21",
   "metadata": {
    "id": "db0a3d21"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A TransformerDecoder is a stack of multiple TransformerDecoderCells and a Layer Norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: How many TransformerDecoderCells in stack\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        # Construct a nn.ModuleList to store a stack of TranformerDecoderCells. Check the documentation here of how to use it.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n",
    "        \n",
    "        # At the same time, define a layer normalization layer to process the output of the entire encoder.                                           #\n",
    " \n",
    "        self.decoder_cells = nn.ModuleList([TransformerDecoderCell(input_dim, num_heads, ff_dim, dropout) for i in range(num_cells)])\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask=None, tgt_mask=None):            \n",
    "        \"\"\"\n",
    "        Inputs: \n",
    "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
    "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
    "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
    "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
    "        \n",
    "        Return:\n",
    "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
    "        \"\"\"\n",
    "        \n",
    "        y = None\n",
    "        # Feed x into the stack of TransformerDecoderCells and then normalize the output with layer norm.                                   #\n",
    "        y = x\n",
    "        for cell in self.cells:\n",
    "          y = cell(y, encoder_output, src_mask, tgt_mask)\n",
    "        y = self.layer_norm(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7e67d",
   "metadata": {
    "id": "0af7e67d"
   },
   "outputs": [],
   "source": [
    "dec_feats = torch.randn((3, 10, 16))\n",
    "dec_mask = torch.randn((3, 1, 10, 10)) > 0.5\n",
    "\n",
    "enc_feats = torch.randn((3, 12, 16))\n",
    "enc_mask = torch.randn((3, 1, 1, 12)) > 0.5\n",
    "\n",
    "model = TransformerDecoder(16, 2, 32, 2, 0.1)\n",
    "z = model(dec_feats, enc_feats, enc_mask, dec_mask)\n",
    "assert len(z.shape) == len(dec_feats.shape)\n",
    "for dim_z, dim_x in zip(z.shape, dec_feats.shape):\n",
    "    assert dim_z == dim_x\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0615f",
   "metadata": {
    "id": "08f0615f"
   },
   "source": [
    "### <font size='4'>Implement a Transformer-based Sequence-to-sequence model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7dabf",
   "metadata": {
    "id": "cad7dabf"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based sequence-to-sequence model.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            num_encoder_layers: int, num_decoder_layers: int, embed_dim: int,\n",
    "            num_heads: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "            trx_ff_dim: int = 512, dropout: float = 0.1, pad_token: int=0\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - num_encoder_layers: How many TransformerEncoderCell in stack\n",
    "        - num_decoder_layers: How many TransformerDecoderCell in stack\n",
    "        - embed_dim: Word embeddings dimension\n",
    "        - num_heads: Number of attention heads\n",
    "        - src_vocab_size: Number of tokens in the source language vocabulary\n",
    "        - tgt_vocab_size: Number of tokens in the target language vocabulary\n",
    "        - trx_ff_dim: Hidden dimension in the feedforward network\n",
    "        - dropout: Dropout ratio\n",
    "        \"\"\"\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Word embeddings for both the source and target languages\n",
    "        self.src_token_embed = nn.Embedding(src_vocab_size, embed_dim, padding_idx=pad_token)\n",
    "        self.tgt_token_embed = nn.Embedding(tgt_vocab_size, embed_dim, padding_idx=pad_token)\n",
    "        \n",
    "        # Define the positional encoding, encoder, decoder, and the output layer. Think of how many classes are in the output layer.               #\n",
    "        # Positional encoding for both the source and target languages\n",
    "        self.src_positional_encoding = PositionalEncoding(embed_dim, dropout)\n",
    "        self.tgt_positional_encoding = PositionalEncoding(embed_dim, dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            input_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=trx_ff_dim,\n",
    "            num_cells=num_encoder_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Transformer decoder\n",
    "        self.decoder = TransformerDecoder(\n",
    "            input_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=trx_ff_dim,\n",
    "            num_cells=num_decoder_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "        \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - src: Tensor of BxLe, word indexes in the source language\n",
    "        - tgt: Tensor of BxLd, word indexes in the target language\n",
    "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
    "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
    "        \n",
    "        Return:\n",
    "        - y: Tensor of BxLdxK. K is the number of classes in the output.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get word embeddings. Not they are scaled.\n",
    "        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n",
    "        tgt_embed = self.tgt_token_embed(tgt) * math.sqrt(self.embed_dim)\n",
    "        \n",
    "        logits = None\n",
    "        # Add positional encodings to the word embeddings. \n",
    "        # Feed them then  to the encoder and decoder, respectively. Get the logits finally.\n",
    "        \n",
    "        # Get word embeddings. Not they are scaled.\n",
    "        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n",
    "        tgt_embed = self.tgt_token_embed(tgt) * math.sqrt(self.embed_dim)\n",
    "        \n",
    "        # Add positional encodings to the word embeddings\n",
    "        src_embed = self.src_positional_encoding(src_embed)\n",
    "        tgt_embed = self.tgt_positional_encoding(tgt_embed)\n",
    "        \n",
    "        # Pass the source and target embeddings through the encoder and decoder, respectively.\n",
    "        encoder_output = self.encoder(src_embed, src_mask)\n",
    "        decoder_output = self.decoder(tgt_embed, encoder_output, tgt_mask, src_mask)\n",
    "        \n",
    "        # Get the logits\n",
    "        logits = self.output_layer(decoder_output)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X5y6pWsfEfr1",
   "metadata": {
    "id": "X5y6pWsfEfr1"
   },
   "source": [
    "The number of output layers in the code is 1, and it is defined in the output_layer attribute of the Seq2SeqTransformer class using nn.Linear with tgt_vocab_size as the number of output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6d873",
   "metadata": {
    "id": "5ef6d873"
   },
   "outputs": [],
   "source": [
    "src_vocab_size = 10\n",
    "src = torch.arange(src_vocab_size).view(1, -1)\n",
    "src = torch.cat((src, src), dim=0)\n",
    "src_mask = torch.randn((2, 1, 1, src_vocab_size)) > 0.5\n",
    "\n",
    "tgt_vocab_size = 12\n",
    "tgt = torch.arange(tgt_vocab_size).view(1, -1)\n",
    "tgt = torch.cat((tgt, tgt), dim=0)\n",
    "tgt_mask = torch.randn((2, 1, tgt_vocab_size, tgt_vocab_size)) > 0.5\n",
    "\n",
    "model = Seq2SeqTransformer(2, 2, 16, 2, src_vocab_size, tgt_vocab_size, 32, 0.1, 0)\n",
    "z = model(src, tgt, src_mask, tgt_mask)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922361b2",
   "metadata": {
    "id": "922361b2"
   },
   "source": [
    "### Create Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a73cc5",
   "metadata": {
    "id": "c9a73cc5"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "\n",
    "def create_mask(src, tgt, pad_token=0):\n",
    "    src_mask = (src != pad_token).unsqueeze(-2).unsqueeze(1)\n",
    "    \n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "    tgt_mask = (tgt != pad_token).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & subsequent_mask(tgt.shape[1]).type_as(tgt_mask.data)\n",
    "\n",
    "    return src_mask, tgt_mask.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd199b2",
   "metadata": {
    "id": "ebd199b2"
   },
   "outputs": [],
   "source": [
    "# Let's visualize what the target mask looks like\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0].numpy())\n",
    "\n",
    "x = torch.arange(src_vocab_size).view(1, -1)\n",
    "x = torch.cat((x, x), dim=0)\n",
    "src_mask, tgt_mask = create_mask(x, x)\n",
    "print(src_mask.shape, tgt_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3815f",
   "metadata": {
    "id": "efd3815f"
   },
   "source": [
    "### Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276afbc7",
   "metadata": {
    "id": "276afbc7"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# # Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    " \n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator \n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac308f",
   "metadata": {
    "id": "e7ac308f"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(\n",
    "        token_transform[ln], #Tokenization\n",
    "        vocab_transform[ln], #Numericalization\n",
    "        tensor_transform # Add BOS/EOS and create tensor\n",
    "    )\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch.transpose(0, 1), tgt_batch.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052976db",
   "metadata": {
    "id": "052976db"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "for idx, (src, tgt) in enumerate(train_dataloader):\n",
    "    if idx > 2:\n",
    "        break\n",
    "    print('src: {}, tgt: {}'.format(src.shape, tgt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9d060",
   "metadata": {
    "id": "76e9d060"
   },
   "source": [
    "### <font size='4'>Define the Model and Loss Function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfcf01",
   "metadata": {
    "id": "b1cfcf01"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMBED_SIZE = 512\n",
    "NUM_ATTN_HEADS = 8\n",
    "FF_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "# Define the model and loss function.                               \n",
    "# Note that this time we will generate tokens, where some of them in the training time are from paddings.\n",
    "#  We don't want to penalize the model if the output at such positions are wrong.     \n",
    "# You can use the `ignore_index` in a loss function to suppress loss computation if the ground-truth label is equal to the given value.\n",
    "# Check here for more details https://pytorch.org/docs/stable/nn.html#loss-functions.\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    embed_dim=EMBED_SIZE,\n",
    "    num_heads=NUM_ATTN_HEADS,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    trx_ff_dim=FF_DIM,\n",
    "    dropout=0.1,\n",
    "    pad_token=PAD_IDX\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), \n",
    "    lr=0.0001, \n",
    "    betas=(0.9, 0.98), \n",
    "    eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14426704",
   "metadata": {
    "id": "14426704"
   },
   "source": [
    "### Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0266e",
   "metadata": {
    "cellView": "form",
    "id": "47d0266e"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
    "        src_mask = src_mask.to(device)\n",
    "        tgt_mask = tgt_mask.to(device)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        \n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# You should be able to get train loss around 1.5 and val loss around 2.2\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660291b",
   "metadata": {
    "id": "3660291b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
